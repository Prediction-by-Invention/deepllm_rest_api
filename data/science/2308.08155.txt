AutoGen Enabling Next Gen LLM Applications via Multi Agent Conversation.
Figure 1 AutoGen enables diverse LLM based applications using multi agent conversations.
AutoGen agents are conversable customizable and can be based on LLMs tools humans or even a combination of them.
Agents can converse to solve tasks.
They can form a chat potentially with humans in the loop.
The framework supports flexible conversation patterns.
AutoGen2 is an open source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks.
AutoGen agents are customizable conversable and can operate in various modes that employ combinations of LLMs human inputs and tools.
Using AutoGen developers can also flexibly define agent interaction behaviors.
Both natural language and computer code can be used to program flexible conversation patterns for different applications.
AutoGen serves as a generic framework for building diverse applications of various complexities and LLM capacities.
Email auto com 2https github com microsoft autogen.
Here is the revised code please plot % change.
Plot a chart of META and TESLA stock price change YTD Output $ MonthOutput % MonthError package yfinanceis not installedSorry.
Please first pip install yfinanceand then execute the Agent Chat.
Large language models LLMs are becoming a crucial building block in developing powerful agents that utilize LLMs for reasoning tool usage and adapting to new observations Yao et al 2022 Xi et al 2023 Wang et al 2023b in many real world tasks.
Given the expanding tasks that could benefit from LLMs and the growing task complexity an intuitive approach to scale up the power of agents is to use multiple agents that cooperate.
Prior work suggests that multiple agents can help encourage divergent thinking Liang et al 2023 improve factuality and reasoning Du et al 2023 and provide validation Wu et al 2023.
In light of the intuition and early evidence of promise it is intriguing to ask the following question how can we facilitate the development of LLM applications that could span a broad spectrum of domains and complexities based on the multi agent approach.
There are at least three reasons confirming its general feasibility and utility thanks to recent advances in LLMs First because chatoptimized LLMs e g GPT 4 show the ability to incorporate feedback LLM agents can cooperate through conversations with each other or human s e g a dialog where agents provide and seek reasoning observations critiques and validation.
Second because a single LLM can exhibit a broad range of capabilities especially when configured with the correct prompt and inference settings conversations between differently configured agents can help combine these broad LLM capabilities in a modular and complementary manner.
Third LLMs have demonstrated ability to solve complex tasks when the tasks are broken into simpler subtasks.
Multi agent conversations can enable this partitioning and integration in an intuitive manner.
How can we leverage the above insights and support different applications with the common requirement of coordinating multiple agents potentially backed by LLMs humans or tools exhibiting different capacities.
We desire a multi agent conversation framework with generic abstraction and effective implementation that has the flexibility to satisfy different application needs.
Achieving this requires addressing two critical questions 1 How can we design individual agents that are capable reusable customizable and effective in multi agent collaboration.
2 How can we develop a straightforward unified interface that can accommodate a wide range of agent conversation patterns.
In practice applications of varying complexities may need distinct sets of agents with specific capabilities and may require different conversation patterns such as single or multi turn dialogs different human involvement modes and static vs dynamic conversation.
Moreover developers may prefer the flexibility to program agent interactions in natural language or code.
Failing to adequately address these two questions would limit the scope of applicability and generality.
While there is contemporaneous exploration of multi agent approaches 3 we present AutoGen a generalized multi agent conversation framework Figure 1 based on the following new concepts.
1 Customizable and conversable agents.
AutoGen uses a generic design of agents that can leverage LLMs human inputs tools or a combination of them.
The result is that developers can easily and quickly create agents with different roles e g agents to write code execute code wire in human feedback validate outputs etc by selecting and configuring a subset of built in capabilities.
The backend can also be readily extended to allow more custom behaviors.
To make these agents suitable for multi agent conversation every agent is made conversable they can receive react and respond to messages.
When configured properly an agent can hold multiple turns of conversations with other agents autonomously or solicit human inputs at certain rounds enabling human agency and automation.
The conversable agent design leverages the strong capability of the most advanced LLMs in taking feedback and making progress via chat and also allows combining capabilities of LLMs in a modular fashion.
A fundamental insight of AutoGen is to simplify and unify complex LLM application workflows as multi agent conversations.
So AutoGen adopts a programming paradigm centered around these inter agent conversations.
We refer to this paradigm as conversation programming which streamlines the development of intricate applications via two primary steps 1 defining a set of conversable agents with specific capabilities and roles as described above 2 programming the interaction behavior between agents via conversationcentric computation and control.
Both steps can be achieved via a fusion of natural and programming languages to build applications with a wide range of conversation patterns and agent behaviors.
AutoGen provides ready to use implementations and also allows easy extension and experimentation for both steps.
3We refer to Appendix A for a detailed discussion.
AutoGen also provides a collection of multi agent applications created using conversable agents and conversation programming.
These applications demonstrate how AutoGen can easily support applications of various complexities and LLMs of various capabilities.
Moreover we perform both evaluation on benchmarks and a pilot study of new applications.
The results show that AutoGen can help achieve outstanding performance on many tasks and enable innovative ways of using LLMs while reducing development effort.
To reduce the effort required for developers to create complex LLM applications across various domains a core design principle of AutoGen is to streamline and consolidate multi agent workflows using multi agent conversations.
This approach also aims to maximize the reusability of implemented agents.
This section introduces the two key concepts of AutoGen conversable agents and conversation programming.
In AutoGen a conversable agent is an entity with a specific role that can pass messages to send and receive information to and from other conversable agents e g to start or continue a conversation.
The agents can act according to programmed behavior patterns described next.
Agent capabilities powered by LLMs humans and tools.
Since an capabilities directly influence how it processes and responds to messages AutoGen allows flexibility to endow its agents with various capabilities.
AutoGen supports many common composable capabilities for agents including.
LLM backed agents exploit many capabilities of advanced LLMs such as role playing implicit state inference and progress making conditioned on conversation history providing feedback adapting from feedback and coding.
These capabilities can be combined in different ways via novel prompting techniques4 to increase an skill and autonomy.
AutoGen also offers enhanced LLM inference features such as result caching error handling message templating etc via an enhanced LLM inference layer.
Human involvement is desired or even essential in many LLM applications.
AutoGen lets a human participate in agent conversation via humanbacked agents which could solicit human inputs at certain rounds of a conversation depending on the agent configuration.
The default user proxy agent allows configurable human involvement levels and patterns e g frequency and conditions for requesting human input including the option for humans to skip providing input.
Tool backed agents have the capability to execute tools via code execution or function execution.
For example the default user proxy agent in AutoGen is able to execute code suggested by LLMs or make LLM suggested function calls.
Based on application specific needs each agent can be configured to have a mix of basic back end types to display complex behavior in multi agent conversations.
AutoGen allows easy creation of agents with specialized capabilities and roles by reusing or extending the built in agents.
The yellow shaded area of Figure 2 provides a sketch of the built in agents in AutoGen.
The ConversableAgent class is the highest level agent abstraction and by default can use LLMs humans and tools.
The AssistantAgent and UserProxyAgent are two pre configured ConversableAgent subclasses each representing a common usage mode i e acting as an AI assistant backed by LLMs and acting as a human proxy to solicit human input or execute code function calls backed by humans and or tools.
In the example on the right hand side of Figure 1 an LLM backed assistant agent and a tool and human backed user proxy agent are deployed together to tackle a task.
Here the assistant agent generates a solution with the help of LLMs and passes the solution to the user proxy agent.
Then the user proxy agent solicits human inputs or executes the code and passes the results as feedback back to the assistant.
4Appendix C presents an example of such novel prompting techniques which empowers the default LLM backed assistant agent in AutoGen to converse with other agents in multi step problem solving.
Figure 2 Illustration of how to use AutoGen to program a multi agent conversation.
The top subfigure illustrates the built in agents provided by AutoGen which have unified conversation interfaces and can be customized.
The middle sub figure shows an example of using AutoGen to develop a two agent system with a custom reply function.
The bottom sub figure illustrates the resulting automated agent chat from the two agent system during program execution.
By allowing custom agents that can converse with each other conversable agents in AutoGen serve as a useful building block.
However to develop applications where agents make meaningful progress on tasks developers also need to be able to specify and mold these multi agent conversations.
As a solution to the above problem AutoGen utilizes conversation programming a paradigm that considers two concepts the first is computation the actions agents take to compute their response in a multi agent conversation.
And the second is control flow the sequence or conditions under which these computations happen.
As we will show in the applications section the ability to program these helps implement many flexible multi agent conversation patterns.
In AutoGen these computations are conversation centric.
An agent takes actions relevant to the conversations it is involved in and its actions result in message passing for consequent conversations unless a termination condition is satisfied.
Similarly control flow is conversation driven the participating decisions on which agents to send messages to and the procedure of computation are functions of the inter agent conversation.
This paradigm helps one to reason intuitively about a complex workflow as agent action taking and conversation message passing between agents.
Figure 2 provides a simple illustration.
The bottom sub figure shows how individual agents perform their role specific conversation centric computations to generate responses e g via LLM inference calls and code execution.
The middle sub figure demonstrates a conversation based control flow.
When the assistant receives a message the user proxy agent typically sends the human input as a reply.
If there is no input it executes any code in the message instead.
2 Initiate Conversations A a chart of META and TESLA stock price change YTD B Assistant BUser Proxy AAutoGenAgentsDeveloper This funcwill be invoked in B def msg ouput not ouput if msg includes code output execute msg return are a helpful AI the following cases suggest python Note when no reply funcis registered a list of default reply functions will be used.
Please first pip install yfinanceand then executeConversation Driven Control Resulting Automated Agent Chat 2 Register a Custom Reply Func 1 1 Define Agents Unified Conversation Interfaces AutoGen features the following design patterns to facilitate conversation programming.
1 Unified interfaces and auto reply mechanisms for automated agent chat.
Agents in AutoGen have unified conversation interfaces for performing the corresponding conversationcentric computation including a send receive function for sending receiving messages and a generate reply function for taking actions and generating a response based on the received message.
AutoGen also introduces and by default adopts an agent auto reply mechanism to realize conversation driven control Once an agent receives a message from another agent it automatically invokes generate reply and sends the reply back to the sender unless a termination condition is satisfied.
AutoGen provides built in reply functions based on LLM inference code or function execution or human input.
One can also register custom reply functions to customize the behavior pattern of an agent e g chatting with another agent before replying to the sender agent.
Under this mechanism once the reply functions are registered and the conversation is initialized the conversation flow is naturally induced and thus the agent conversation proceeds naturally without any extra control plane i e a special module that controls the conversation flow.
For example with the developer code in the blue shaded area marked of Figure 2 one can readily trigger the conversation among the agents and the conversation would proceed automatically as shown in the dialog box in the grey shaded area marked of Figure.
2 The auto reply mechanism provides a decentralized modular and unified way to define the workflow.
2 Control by fusion of programming and natural language.
AutoGen allows the usage of programming and natural language in various control flow management patterns.
1 Naturallanguage control via LLMs.
In AutoGen one can control the conversation flow by prompting the LLM backed agents with natural language.
For instance the default system message of the built in AssistantAgent in AutoGen uses natural language to instruct the agent to fix errors and generate code again if the previous result indicates there are errors.
It also guides the agent to confine the LLM output to certain structures making it easier for other tool backed agents to consume.
For example instructing the agent to reply with when all tasks are completed to terminate the program.
In AutoGen Python code can be used to specify the termination condition human input mode and tool execution logic e g the max number of auto replies.
3 Control transition between natural and programming language.
AutoGen also supports flexible control transition between natural and programming language.
One can achieve transition from code to natural language control by invoking an LLM inference containing certain control logic in a customized reply function or transition from natural language to code control via LLM proposed function calls Eleti et al 2023.
In the conversation programming paradigm one can realize multi agent conversations of diverse patterns.
In addition to static conversation with predefined flow AutoGen also supports dynamic conversation flows with multiple agents.
AutoGen provides two general ways to achieve this.
1 Customized generate reply function within the customized generate reply function one agent can hold the current conversation while invoking conversations with other agents depending on the content of the current message and context.
2 Function calls In this approach LLM decides whether or not to call a particular function depending on the conversation status.
By messaging additional agents in the called functions the LLM can drive dynamic multi agent conversation.
In addition AutoGen supports more complex dynamic group chat via built in GroupChatManager which can dynamically select the next speaker and then broadcast its response to other agents.
We demonstrate six applications using AutoGen see Figure 3 to illustrate its potential in simplifying the development of high performance multi agent applications.
Together these criteria showcase role in advancing the LLM application landscape.
Figure 3 Six examples of diverse applications built using AutoGen.
Their conversation patterns show flexibility and power.
This section demonstrates how AutoGen can help develop LLM applications for math problem solving showcasing strong performance and flexibility in supporting various problem solving paradigms.
We are able to build a system for autonomous math problem solving by directly reusing two built in agents from AutoGen.
We perform evaluations over 120 randomly selected level 5 problems and on the entire5 test dataset from MATH.
The results show that the built in agents from AutoGen already yield better performance out of the box compared to the alternative approaches even including the commercial ones.
2 We also showcase a human in the loop problem solving process with the help of AutoGen.
We demonstrate that this system can effectively incorporate human inputs to solve challenging problems that cannot be solved without humans.
3 We further demonstrate a novel scenario where multiple human users can participate in the conversations during the problem solving process.
Our experiments and case studies for these scenarios show that AutoGen enables better performance or new experience compared to other solutions we experimented with.
A2 Retrieval Augmented Code Generation and Question Answering.
Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic limitations of LLMs by incorporating external documents.
In this section we employ AutoGen to build a Retrieval Augmented Generation RAG system Lewis et al 2020 Parvez et al 2021 named Retrieval augmented Chat.
The system consists of two agents a Retrieval augmented User Proxy agent and a Retrieval augmented Assistant agent both of which are extended from built in agents from AutoGen.
The Retrieval augmented User Proxy includes a vector database Chroma.
5We did not evaluate ChatGPT on the whole dataset since it requires substantial manual effort and is restricted by its hourly message number limitation.
Retrieval augmented ChatRetrieval augmentedAssistantRetrieval augmentedUser ProxyChess BoardHuman AI Chess Player AHuman AI Chess Player BStudentAssistantAssistantExpertAsk expertBroadcastManagerSpeakA5.
c A3 Performance on ALFWorld.
d A4 Performance on OptiGuide.
a shows that AutoGen agents can be used out of the box to achieve the most competitive performance on math problem solving tasks.
b shows that AutoGen can be used to realize effective retrieval augmentation and realize a novel interactive retrieval feature to boost performance on tasks.
c shows that AutoGen can be used to introduce a three agent system with a grounding agent to improve performance on ALFWorld.
d shows that a multi agent design is helpful in boosting performance in coding tasks that need safeguards.
2023 with SentenceTransformers Reimers Gurevych 2019 as the context retriever.
We evaluate Retrieval augmented Chat in both question answering and code generation scenarios.
In this evaluation we compare our system with DPR Dense Passage Retrieval following an existing evaluation6 practice Adlakha et al 2023.
Leveraging the conversational design and natural language control AutoGen introduces a novel interactive retrieval feature in this application whenever the retrieved context does not contain the information instead of terminating the LLM based assistant would reply I cannot find any information about UPDATE CONTEXT which will invoke more retrieval attempts.
The results show that the interactive retrieval mechanism indeed plays a non trivial role in the process.
We give a concrete example and results using this appealing feature in Appendix D Scenario 2 We further demonstrate how Retrieval augmented Chat aids in generating code based on a given codebase that contains code not included in GPT training data.
6The results of DPR with GPT 3 5 shown in Figure 4b are from Adlakha et al 2023.
a shorthand for GPT 3 5 turbo.
AutoGenChatGPT CodeChatGPT PluginGPT 4Multi AgentDebateLangChainReActMethods01020304050607080Success Ratio % 52 5 % 48 33 % 45 0 % 30 0 % 26 67 % 23 33 % 69 48 % 55 18 % 120 Level 5 problemsWhole DatasetF1RecallMetrics01020304050607080Percentage % 25 88 % 66 65 % 15 12 % 58 56 % 22 79 % 62 59 % AutoGenAuotGen W O interactive retrievalDPRAutoGen 3 agent AutoGen 2 agent ReActMethods020406080100Success Ratio % 69 % 54 % 54 % 77 % 63 % 66 % AverageBest of 3F1RecallMetrics020406080100Percentage % 96 00 % 98 00 % 88 00 % 78 00 % 83 00 % 72 00 % 48 00 % 32 00 % Multi GPT4Single GPT4Multi GPT3 5Single GPT3 5 A3 Decision Making in Text World Environments.
In this subsection we demonstrate how AutoGen can be used to develop effective applications that involve interactive or online decision making.
We perform the study using the ALFWorld Shridhar et al 2021 benchmark which includes a diverse collection of synthetic language based interactive decision making tasks in household environments.
With AutoGen we implemented a two agent system to solve tasks from ALFWorld.
It consists of an LLM backed assistant agent responsible for suggesting plans to conduct a task and an executor agent responsible for executing actions in the ALFWorld environments.
This system integrates ReAct prompting Yao et al 2022 and is able to achieve similar performance.
A common challenge encountered in both ReAct and the AutoGen based two agent system is their occasional inability to leverage basic commonsense knowledge about the physical world.
This deficiency can lead to the system getting stuck in a loop due to repetitive errors.
Fortunately the modular design of AutoGen allows us to address this issue effectively With AutoGen we are able to introduce a grounding agent which supplies crucial commonsense as must find and take the object before you can examine it You must go to where the target object is before you can use it the system exhibits early signs of recurring errors.
It significantly enhances the ability to avoid getting entangled in error loops.
The results show that introducing a grounding agent could bring in a 15 % performance gain on average.
Upon examining the outputs we observe that the grounding agent by delivering background commonsense knowledge at the right junctures significantly mitigated the tendency of the system to persist with a flawed plan thereby avoiding the creation of error loops.
In this subsection we use AutoGen to build a multi agent coding system based on OptiGuide Li et al 2023a a system that excels at writing code to interpret optimization solutions and answer user questions such as exploring the implications of changing a supply chain decision or understanding why the optimizer made a particular choice.
The second sub figure of Figure 3 shows the AutoGen based implementation.
The workflow is as follows the end user sends questions such as if we prohibit shipping from supplier 1 to roastery 2 to the Commander agent.
The Commander coordinates with two assistant agents including the Writer and the Safeguard to answer the question.
The Writer will craft code and send the code to the Commander.
After receiving the code the Commander checks the code safety with the Safeguard if cleared the Commander will use external tools e g Python to execute the code and request the Writer to interpret the execution results.
The Commander then provides this concluding answer to the end user.
If at a particular step there is an exception e g security red flag raised by Safeguard the Commander redirects the issue back to the Writer with debugging information.
With AutoGen the core workflow code for OptiGuide was reduced from over 430 lines to 100 lines leading to significant productivity improvement.
We provide a detailed comparison of user experience with ChatGPT Code Interpreter and AutoGen based OptiGuide in Appendix D where we show that AutoGen based OptiGuide could save around 3x of time and reduce user interactions by 3 5 times on average.
We also conduct an ablation showing that multi agent abstraction is necessary.
Specifically we construct a single agent approach where a single agent conducts both the code writing and safeguard processes.
We tested the single and multi agent approaches on a dataset of 100 coding tasks which is crafted to include equal numbers of safe and unsafe tasks.
Evaluation results as reported in Figure 4d show that the multi agent design boosts the F 1 score in identifying unsafe code by 8 % with GPT 4 and 35 % with GPT 3 5 turbo.
7Results of ReAct are obtained by directly running its official code with default settings.
AutoGen provides native support for a dynamic group chat communication pattern in which participating agents share the same context and converse with the others in a dynamic manner instead of following a pre defined order.
Dynamic group chat relies on ongoing conversations to guide the flow of interaction among agents.
These make dynamic group chat ideal for situations where collaboration without strict communication order is beneficial.
For the dynamic speaker selection component we use a role play style prompt.
Through a pilot study on 12 manually crafted complex tasks we observed that compared to a prompt that is purely based on the task utilizing a role play prompt often leads to more effective consideration of both conversation context and role alignment during the problem solving and speaker selection process.
Consequently this leads to a higher success rate and fewer LLM calls.
It features built in agents for players which can be human or LLM and a third party board agent to provide information and validate moves based on standard rules.
With AutoGen we enabled two essential features 1 Natural flexible and engaging game dynamics enabled by the customizable agent design in AutoGen.
Conversational Chess supports a range of game play patterns including AI AI AI human and human human with seamless switching between these modes during a single game.
An illustrative example of these entertaining game dynamics can be found in Figure 15 Appendix D 2 Grounding which is a crucial aspect to maintain game integrity.
During gameplay the board agent checks each proposed move for legality if a move is invalid the agent responds with an error prompting the player agent to re propose a legal move before continuing.
This process ensures that only valid moves are played and helps maintain a consistent gaming experience.
As an ablation study we removed the board agent and instead only relied on a relevant prompt should make sure both you and the opponent are making legal to ground their move.
The results highlighted that without the board agent illegitimate moves caused game disruptions.
The modular design offered flexibility allowing swift adjustments to the board agent in response to evolving game rules or varying chess rule variants.
We introduced an open source library AutoGen that incorporates the paradigms of conversable agents and conversation programming.
This library utilizes capable agents that are well suited for multi agent cooperation.
It features a unified conversation interface among the agents along with an auto reply mechanisms which help establish an agent interaction interface that capitalizes on the strengths of chat optimized LLMs with broad capabilities while accommodating a wide range of applications.
AutoGen serves as a general framework for creating and experimenting with multiagent systems that can easily fulfill various practical requirements such as reusing customizing and extending existing agents as well as programming conversations between them.
Our experiments as detailed in Section 3 demonstrate that this approach offers numerous benefits.
The adoption of AutoGen has resulted in improved performance over state of the art approaches reduced development code and decreased manual burden for existing applications.
It offers flexibility to developers as demonstrated in A1 scenario 3 A5 and A6 where AutoGen enables multi agent chats to follow a dynamic pattern rather than fixed back and forth interactions.
It allows humans to engage in activities alongside multiple AI agents in a conversational manner.
Despite the complexity of these applications most involving more than two agents or dynamic multi turn agent cooperation the implementation based on AutoGen remains straightforward.
Dividing tasks among separate agents promotes modularity.
Furthermore since each agent can be developed tested and maintained separately this approach simplifies overall development and code management.
Although this work is still in its early experimental stages it paves the way for numerous future directions and research opportunities.
For instance we can explore effective integration of existing agent implementations into our multi agent framework and investigate the optimal balance between automation and human control in multi agent workflows.
As we further develop and refine AutoGen we aim to investigate which strategies such as agent topology and conversation patterns lead to the most effective multi agent conversations while optimizing the overall efficiency among other factors.
While increasing the number of agents and other degrees of freedom presents opportunities for tackling more complex problems it may also introduce new safety challenges that require additional studies and careful consideration.
We provide more discussion in Appendix B including guidelines for using AutoGen and direction of future work.
We hope AutoGen will help improve many LLM applications in terms of speed of development ease of experimentation and overall effectiveness and safety.
We actively welcome contributions from the broader community.
There are several potential ethical considerations that could arise from the development and use of the AutoGen framework.
Privacy and Data Protection The framework allows for human participation in conversations between agents.
It is important to ensure that user data and conversations are protected and that developers use appropriate measures to safeguard privacy.
Bias and Fairness LLMs have been shown to exhibit biases present in their training data Navigli et al 2023.
When using LLMs in the AutoGen framework it is crucial to address and mitigate any biases that may arise in the conversations between agents.
Developers should be aware of potential biases and take steps to ensure fairness and inclusivity.
Accountability and Transparency As discussed in the future work section as the framework involves multiple agents conversing and cooperating it is important to establish clear accountability and transparency mechanisms.
Users should be able to understand and trace the decision making process of the agents involved in order to ensure accountability and address any potential issues or biases.
Trust and Reliance AutoGen leverages human understanding and intelligence while providing automation through conversations between agents.
It is important to consider the impact of this interaction on user experience trust and reliance on AI systems.
Clear communication and user education about the capabilities and limitations of the system will be essential Cai et al 2019.
Unintended Consequences As discussed before the use of multi agent conversations and automation in complex tasks may have unintended consequences.
In particular allowing LLM agents to make changes in external environments through code execution or function calls such as installing packages could be risky.
Developers should carefully consider the potential risks and ensure that appropriate safeguards are in place to prevent harm or negative outcomes.
The work presented in this report was made possible through discussions and feedback from Peter Lee Johannes Gehrke Eric Horvitz Steven Lucco Umesh Madan Robin Moeur Piali Choudhury Saleema Amershi Adam Fourney Victor Dibia Guoqing Zheng Corby Rosset Ricky Loynd Ece Kamar Rafah Hosn John Langford Ida Momennejad Brian Krabach Taylor Webb Shanka Subhra Mondal Wei ge Chen Robert Gruen Yinan Li Yue Wang Suman Nath Tanakorn Leesatapornwongsa Xin Wang Shishir Patil Tianjun Zhang Saehan Jo Ishai Menache Kontantina Mellou Runlong Zhou Feiran Jia Hamed Khanpour Hamid Palangi Srinagesh Sharma Julio Albinati Cortez Amin Saied Yuzhe Ma Dujian Ding Linyong Nan Prateek Yadav Shannon Shen Ankur Mallick Mark Lars Liden Tianwei Yue Julia Kiseleva Anastasia Razdaibiedina and Luciano Del Corro.
Qingyun Wu would like to acknowledge the funding and research support from the College of Information Science and Technology at Penn State University.
Vaibhav Adlakha Parishad BehnamGhader Xing Han Lu Nicholas Meade and Siva Reddy.
Evaluating correctness and faithfulness of instruction following models for question answering.
arXiv preprint arXiv 2307 16877 2023.
Guidelines for human ai interaction.
In Proceedings of the 2019 chi conference on human factors in computing systems 2019.
Con crete problems in ai safety 2016.
https github com yoheinakajima babyagi 2023.
Carrie J Cai Samantha Winter David F Steiner Lauren Wilcox and Michael Terry.
Uncovering the onboarding needs of medical practitioners for human ai collaborative decisionmaking.
Proceedings of the ACM on Human Computer Interaction 2019.
Tianle Cai Xuezhi Wang Tengyu Ma Xinyun Chen and Denny Zhou.
arXiv preprint arXiv 2305 17126 2023.
https github com chroma core chroma 2023.
LIDA A tool for automatic generation of grammar agnostic visualizations and infographics using large language models.
In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics Volume 3 System Demonstrations Toronto Canada July 2023.
Self collaboration code generation via chatgpt.
Yilun Du Shuang Li Antonio Torralba Joshua B Tenenbaum and Igor Mordatch.
Improving factuality and reasoning in language models through multiagent debate.
arXiv preprint arXiv 2305 14325 2023.
Atty Eleti Jeff Harris and Logan Kilpatrick.
Function calling and other api updates.
openai com blog function calling and other api updates 2023.
https github com guidance ai guidance 2023.
Dan Hendrycks Collin Burns Saurav Kadavath Akul Arora Steven Basart Eric Tang Dawn Song and Jacob Steinhardt.
Measuring mathematical problem solving with the math dataset.
arXiv preprint arXiv 2103 03874 2021.
Metagpt Meta programming for multi agent collaborative framework.
arXiv preprint arXiv 2308 00352 2023.
Principles of mixed initiative user interfaces.
In Proceedings of the SIGCHI conference.
on Human Factors in Computing Systems 1999.
Geunwoo Kim Pierre Baldi and Stephen McAleer.
Language models can solve computer tasks.
arXiv preprint arXiv 2303 17491 2023.
Natural questions a benchmark for question answering research.
Transactions of the Association for Computational Linguistics 2019.
https python langchain com en latest index.
Mike Lewis Denis Yarats Yann N Dauphin Devi Parikh and Dhruv Batra.
end to end learning for negotiation dialogues.
arXiv preprint arXiv 1706 05125 2017.
Retrieval augmented generation for knowledge intensive nlp tasks.
Advances in Neural Information Processing Systems 2020.
Beibin Li Konstantina Mellou Bo Zhang Jeevan Pathuri and Ishai Menache.
arXiv preprint arXiv 2307 03875 2023a.
Guohao Li Hasan Abed Al Kader Hammoud Hani Itani Dmitrii Khizbullin and Bernard Ghanem.
Camel Communicative agents for exploration of large scale language model society 2023b.
Encouraging divergent thinking in large language models through multiagent debate 2023.
Evan Zheran Liu Kelvin Guu Panupong Pasupat Tianlin Shi and Percy Liang.
Reinforcement learning on web interfaces using workflow guided exploration.
arXiv preprint arXiv 1802 08802 2018.
URL https github com jerryjliu.
Volodymyr Mnih Koray Kavukcuoglu David Silver Alex Graves Ioannis Antonoglou Daan Wierstra and Martin Riedmiller.
Playing atari with deep reinforcement learning.
arXiv preprint arXiv 1312 5602 2013.
Roberto Navigli Simone Conia and Ross.
Biases in large language models Origins inven tory and discussion.
ACM Journal of Data and Information Quality 2023.
https openai com blog chatgpt plugins 2023.
Joon Sung Park Joseph C Carrie J Cai Meredith Ringel Morris Percy Liang and Michael S Bernstein.
Generative agents Interactive simulacra of human behavior.
arXiv preprint arXiv 2304 03442 2023.
Md Rizwan Parvez Wasi Uddin Ahmad Saikat Chakraborty Baishakhi Ray and Kai Wei Chang.
Retrieval augmented code generation and summarization.
arXiv preprint arXiv 2108 11601 2021.
Shishir G Patil Tianjun Zhang Xin Wang and Joseph E Gonzalez.
arXiv preprint arXiv 2305 15334 2023.
Nils Reimers and Iryna Gurevych.
Sentence bert Sentence embeddings using siamese bertnetworks.
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics 11 2019.
URL https arxiv org abs 1908 10084.
https github com microsoft semantic kernel.
Bokui Shen Fei Xia Chengshu Li Roberto Linxi Fan Guanzhi Wang Claudia Shyamal Buch Sanjana Srivastava Lyne Tchapmi et al igibson 1 0 A simulation environment for interactive tasks in large realistic scenes.
In 2021 IEEE RSJ International Conference on Intelligent Robots and Systems IROS.
Tianlin Shi Andrej Karpathy Linxi Fan Jonathan Hernandez and Percy Liang.
World of bits An open domain platform for web based agents.
In International Conference on Machine Learning.
Mohit Shridhar Xingdi Yuan Marc Alexandre Yonatan Bisk Adam Trischler and Matthew Hausknecht.
ALFWorld Aligning Text and Embodied Environments for Interactive Learning.
In Proceedings of the International Conference on Learning Representations ICLR 2021.
URL https arxiv org abs 2010 03768.
Starcraft ii A new challenge for reinforcement learning.
arXiv preprint arXiv 1708 04782 2017.
Flaml A fast and lightweight automl.
Proceedings of Machine Learning and Systems 2021.
Guanzhi Wang Yuqi Xie Yunfan Jiang Ajay Mandlekar Chaowei Xiao Yuke Zhu Linxi Fan and Anima Anandkumar.
Voyager An open ended embodied agent with large language models.
arXiv preprint arXiv 2305 16291 2023a.
A survey on large language model based autonomous agents.
arXiv preprint arXiv 2308 11432 2023b.
Daniel S Weld and Oren Etzioni.
The first law of robotics a call to arms.
https minimaxir com 2023 07 langchain problem.
Yiran Wu Feiran Jia Shaokun Zhang Qingyun Wu Hangyu Li Erkang Zhu Yue Wang Yin Tat Lee Richard Peng and Chi Wang.
arXiv preprint arXiv 2306 01337 2023.
The rise and potential of large language model based agents A survey.
arXiv preprint arXiv 2309 07864 2023.
React Synergizing reasoning and acting in language models.
arXiv preprint arXiv 2210 03629 2022.
We examine existing LLM based agent systems or frameworks that can be used to build LLM applications.
Note that many of these systems are evolving open source projects so the remarks and statements about them may only be accurate as of the time of writing.
We refer interested readers to detailed LLM based agent surveys Xi et al 2023 Wang et al 2023b.
AutoGPT AutoGPT is an open source implementation of an AI agent that attempts to autonomously achieve a given goal AutoGPT 2023.
It follows a single agent paradigm in which it augments the AI model with many useful tools and does not support multi agent collaboration.
ChatGPT with code interpreter or plugin ChatGPT a conversational AI service or agent can now be used alongside a code interpreter or plugin currently available only under the premium subscription plan ChatGPT Plus OpenAI 2023.
The code interpreter enables ChatGPT to execute code while the plugin enhances ChatGPT with a wide range of curated tools.
LangChain Agents LangChain is a general framework for developing LLM based applications LangChain 2023.
LangChain Agents is a subpackage for using an LLM to choose a sequence of actions.
There are various types of agents in LangChain Agents with the ReAct agent being a notable example that combines reasoning and acting when using LLMs mainly designed for LLMs prior to ChatGPT Yao et al 2022.
All agents provided in LangChain Agents follow a single agent paradigm and are not inherently designed for communicative and collaborative modes.
A significant summary of its limitations can be found in Woolf 2023.
Due to these limitations even the multi agent systems in LangChain e g re implementation of CAMEL are not based on LangChain Agents but are implemented from scratch.
Their connection to LangChain lies in the use of basic orchestration modules provided by LangChain such as AI models wrapped by LangChain and the corresponding interface.
Transformers Agent Transformers Agent HuggingFace 2023 is an experimental naturallanguage API built on the transformers repository.
It includes a set of curated tools and an agent to interpret natural language and use these tools.
Similar to AutoGPT it follows a single agent paradigm and does not support agent collaboration.
AutoGen differs from the single agent systems above by supporting multi agent LLM applications.
BabyAGI BabyAGI BabyAGI 2023 is an example implementation of an AI powered task management system in a Python script.
In this implemented system multiple LLM based agents are used.
For example there is an agent for creating new tasks based on the objective and the result of the previous task an agent for prioritizing the task list and an agent for completing tasks sub tasks.
As a multi agent system BabyAGI adopts a static agent conversation pattern i e a predefined order of agent communication while AutoGen supports both static and dynamic conversation patterns and additionally supports tool usage and human involvement.
CAMEL CAMEL Li et al 2023b is a communicative agent framework.
It demonstrates how role playing can be used to let chat agents communicate with each other for task completion.
It also records agent conversations for behavior analysis and capability understanding.
An Inception prompting technique is used to achieve autonomous cooperation between agents.
Unlike AutoGen CAMEL does not natively support tool usage such as code execution.
Although it is proposed as an infrastructure for multi agent conversation it only supports static conversation patterns while AutoGen additionally supports dynamic conversation patterns.
Multi Agent Debate Two recent works investigate and show that multi agent debate is an effective way to encourage divergent thinking in LLMs Liang et al 2023 and to improve the factuality and reasoning of LLMs Du et al 2023.
In both works multiple LLM inference instances are constructed as multiple agents to solve problems with agent debate.
Each agent is simply an LLM inference instance while no tool or human is involved and the inter agent conversation needs to follow a pre defined order.
These works attempt to build LLM applications with multi agent conversation while AutoGen designed as a generic infrastructure can be used to facilitate this development and enable more applications with dynamic conversation patterns.
MetaGPT MetaGPT Hong et al 2023 is a specialized LLM application based on a multi agent conversation framework for automatic software development.
They assign different roles to GPTs to collaboratively develop software.
They differ from AutoGen by being specialized solutions to a certain scenario while AutoGen is a generic infrastructure to facilitate building applications for various scenarios.
There are a few other specialized single agent or multi agent systems such as Voyager Wang et al 2023a and Generative Agents Park et al 2023 which we skip due to lower relevance.
In Table 1 we summarize differences between AutoGen and the most relevant multi agent systems.
Table 1 Summary of differences between AutoGen and other related multi agent systems.
infrastructure whether the system is designed as a generic infrastructure for building LLM applications.
conversation pattern the types of patterns supported by the implemented systems.
Under the pattern agent topology remains unchanged regardless of different inputs.
AutoGen allows flexible conversation patterns including both static and dynamic patterns that can be customized based on different application needs.
execution capable whether the system can execute LLMgenerated code human involvement whether and how the system allows human participation during the execution process of the system.
AutoGen allows flexible human involvement in multiagent conversation with the option for humans to skip providing inputs.
Aspect Infrastructure Conversation pattern Execution capable Human involvement.
The applications in Section 3 show how AutoGen not only enables new applications but also helps renovate existing ones.
For example in A1 scenario 3 A5 and A6 AutoGen enabled the creation of multi agent conversations that follow a dynamic pattern instead of a fixed back and forth.
And in both A5 and A6 humans can participate in the activities together with multiple other AI agents in a conversational manner.
Similarly A1 A4 show how popular applications can be renovated quickly with AutoGen.
Despite the complexity of these applications most of them involve more than two agents or dynamic multi turn agent cooperation our AutoGen based implementation remains simple demonstrating promising opportunities to build creative applications and a large space for innovation.
In reflecting on why these benefits can be achieved in these applications with AutoGen we believe there are a few reasons.
Ease of use The built in agents can be used out of the box delivering strong performance even.
Modularity The division of tasks into separate agents promotes modularity in the system.
Each agent can be developed tested and maintained independently simplifying the overall development process and facilitating code management.
Programmability AutoGen allows users to extend customize existing agents to develop systems satisfying their specific needs with ease.
For example with AutoGen the core workflow code in A4 is reduced from over 430 lines to 100 lines for a 4x saving.
Allowing human involvement AutoGen provides a native mechanism to achieve human participation and or human oversight.
With AutoGen humans can seamlessly and optionally cooperate with AIs to solve problems or generally participate in the activity.
AutoGen also facilitates interactive user instructions to ensure the process stays on the desired path.
A1 A2 A5 and A6 Collaborative adversarial agent interactions Like many collaborative agent systems Dong et al 2023 agents in AutoGen can share information and knowledge to complement each abilities and collectively arrive at better solutions.
Relevant information is shared among different conversations in a controlled manner preventing distraction or hallucination.
AutoGen supports both patterns enabling effective utilization and augmentation of LLMs.
General Guidelines for Using AutoGen.
Below we give some recommendations for using agents in AutoGen to accomplish a task.
1 Consider using built in agents first.
For example AssistantAgent is pre configured to be backed by GPT 4 with a carefully designed system message for generic problem solving via code.
The UserProxyAgent is configured to solicit human inputs and perform tool execution.
Many problems can be solved by simply combining these two agents.
4 when system message modification is needed consider leveraging the capability to program its conversation flow with natural language.
2 Start with a simple conversation topology.
Consider using the two agent chat or the group chat setup first as they can often be extended with the least code.
4 When developing a new application with UserProxyAgent start with humans always in the loop i e human input mode even if the target operation mode is more autonomous.
This helps evaluate the effectiveness of AssistantAgent tuning the prompt discovering corner cases and debugging.
Once confident with small scale success consider setting.
This enables LLM as a backend and one can either use the LLM or manually generate diverse system messages to simulate different use cases.
5 Despite the numerous advantages of AutoGen agents there could be cases scenarios where other libraries packages could help.
For example 1 For sub tasks that do not have requirements for back and forth trouble shooting multi agent interaction etc a unidirectional no back andforth message exchange pipeline can also be orchestrated with LangChain LangChain 2023 LlamaIndex Liu 2022 Guidance Guidance 2023 Semantic Kernel Semantic Kernel 2023 Gorilla Patil et al 2023 or low level inference API provides an enhanced LLM inference layer at this level Dibia 2023.
2 When existing tools from LangChain etc are helpful one can use them as tool backends for AutoGen agents.
For example one can readily use tools e g Wolfram Alpha from LangChain in AutoGen agent.
3 For specific applications one may want to leverage agents implemented in other libraries packages.
To achieve this one could wrap those agents as conversable agents in AutoGen and then use them to build LLM applications through multi agent conversation.
4 It can be hard to find an optimal operating point among many tunable choices such as the LLM inference configuration.
Blackbox optimization packages like Wang et al 2021 can be used together with AutoGen to automate such tuning.
Designing optimal multi agent workflows Creating a multi agent workflow for a given task can involve many decisions e g how many agents to include how to assign agent roles and agent capabilities how the agents should interact with each other and whether to automate a particular part of the workflow.
There may not exist a one fits all answer and the best solution might depend on the specific application.
This raises important questions For what types of tasks and applications are multi agent workflows most useful.
How do multiple agents help in different applications.
For a given task what is the optimal e g cost effective multi agent workflow.
Creating highly capable agents AutoGen can enable the development of highly capable agents that leverage the strengths of LLMs tools and humans.
Creating such agents is crucial to ensuring that a multi agent workflow can effectively troubleshoot and make progress on a task.
For example we observed that CAMEL another multi agent LLM system cannot effectively solve problems in most cases primarily because it lacks the capability to execute tools or code.
This failure shows that LLMs and multi agent conversations with simple role playing are insufficient and highly capable agents with diverse skill sets are essential.
We believe that more systematic work will be required to develop guidelines for application specific agents to create a large OSS knowledge base of agents and to create agents that can discover and upgrade their skills Cai et al 2023.
Enabling scale safety and human agency Section 3 shows how complex multi agent workflows can enable new applications and future work will be needed to assess whether scaling further can help solve extremely complex tasks.
However as these workflows scale and grow more complex it may become difficult to log and adjust them.
Thus it will become essential to develop clear mechanisms and tools to track and debug their behavior.
Otherwise these techniques risk resulting in incomprehensible unintelligible chatter among agents Lewis et al 2017.
Our work also shows how complex fully autonomous workflows with AutoGen can be useful but fully autonomous agent conversations will need to be used with care.
While the autonomous mode AutoGen supports could be desirable in many scenarios a high level of autonomy can also pose potential risks especially in high risk applications Amodei et al 2016 Weld Etzioni 1994.
As a result building fail safes against cascading failures and exploitation mitigating reward hacking out of control and undesired behaviors maintaining effective human oversight of applications built with AutoGen agents will become important.
While AutoGen provides convenient and seamless involvement of humans through a user proxy agent developers and stakeholders still need to understand and determine the appropriate level and pattern of human involvement to ensure the safe and ethical use of the technology Horvitz 1999 Amershi et al 2019.
C Default System Message for Assistant Agent.
This is an example of conversation programming via natural language.
It contains instructions of different types including role play control flow output confine facilitate automation and grounding.
Figure 5 shows the default system message for the built in assistant agent in AutoGen v0 1 1 where we introduce several new prompting techniques and highlight them accordingly.
When combining these new prompting techniques together we can program a fairly complex conversation even with the simplest two agent conversation topology.
This approach tries to exploit the capability of LLMs in implicit state inference to a large degree.
LLMs do not follow all the instructions perfectly so the design of the system needs to have other mechanisms to handle the exceptions and faults.
Some instructions can have ambiguities and the designer should either reduce them for preciseness or intentionally keep them for flexibility and address the different situations in other agents.
general we observe that GPT 4 follows the instructions better than GPT 3 5 turbo.
SystemMessageYou are a helpful AI assistant.
When you need to collect info use the code to output the info you need for example browse or search the web download read a file print the content of a webpage or a file get the current date time.
When you need to perform some task with code use the code to perform the task and output theresult.
If a plan is not provided explain your plan first.
Be clear which step uses code and which step uses your language skill When using code you must indicate the script type in the code block.
The user cannot provide any other feedback or perform any other action beyond executing the code you suggest.
The user modify your code.
So do not suggest incomplete code which requires users to modify.
use a code block if not intended to be executed by the user If you want the user to save the code in a file before executing it put filename inside the code block as the first line.
include multiple code blocks in one response.
Do not ask users to copy and paste the result.
Instead use function for the output when relevant.
Check the execution result returned by the user If the result indicates there is an error fix the error and output the code again.
Suggest the full code instead of partial code or code changes.
If the error be fixed or if the task is not solved even after the code is executed successfully analyze the problem revisit your assumption collect additional info you need and think of a different approach to try When you find an answer verify the answer carefully.
Include verifiable evidence in your response if possible Reply in the end when everything is done Prompting techniques color code Role Play Control Flow Output Confine Facilitate Automation Grounding D Application Details.
Scenario 1 Autonomous Problem Solving.
We perform both qualitative and quantitative evaluations in this scenario.
For all evaluations we use GPT 4 as the base model and pre install the package in the execution environment.
We compare AutoGen with the following LLMbased agent systems.
AutoGPT The out of box AutoGPT is used.
math resulting in a with auto generated goals.
ChatGPT Plugin We enable the Wolfram Alpha plugin a math computation engine in the Ope nAI web client.
ChatGPT Code Interpreter This is a recent feature in OpenAI web client.
Note that the above two premium features from ChatGPT require a paid subscription to be accessed and are the most competitive commercial systems.
LangChain ReAct Python We use Python agent from LangChain.
set parsing errors and use the default zero shot ReAct prompt.
Multi Agent Debate Liang et al 2023 We modified the code of the multi agent debate to perform evaluation.
By default there are three agents an affirmative agent a negative agent and a moderator.
We also conducted preliminary evaluations on several other multi agent systems including BabyAGI CAMEL and MetaGPT.
For instance when MetaGPT is tasked with solving a math problem it begins developing software to address the problem but most of the time it does not actually solve the problem.
Table 2 Qualitative evaluation of two math problems from the MATH dataset within the autonomous problem solving scenario.
Each LLM based system is tested three times on each of the problems.
This table reports the problem solving correctness and summarizes the reasons for failure.
ChatGPT Code Interpreter LangChain ReAct Multi Agent Debate.
The LLM gives code without the print function so the result is not printed.
The return from Wolfram Alpha contains 2 simplified results including the correct answer but GPT 4 always chooses the wrong answer.
Returns a wrong decimal result.
LangChain gives 3 different wrong answers.
It gives 3 different wrong answers due to calculation errors.
a Evaluation on the first problem that asks to simplify a square root fraction.
ChatGPT Code Interpreter LangChain ReAct Multi Agent Debate.
Failure Reason The final answer from code execution is wrong.
The LLM gives code without the print function so the result is not printed.
For one trial GPT 4 got stuck because it keeps giving wrong queries and has to be stopped.
Another trial simply gives a wrong answer.
It gives 3 different wrong answers.
LangChain gives 3 different wrong answers.
It gives 3 different wrong answers.
b Evaluation on the second number theory problem.
For the qualitative evaluation we utilize two level 5 problems from the MATH dataset testing each problem three times.
The first problem involves simplifying a square root fraction and the second.
problem involves solving a number theory issue.
For the quantitative evaluation we conduct two sets of experiments on the MATH dataset to assess the correctness of these systems 1 an experiment involving 120 level 5 the most challenging level problems including 20 problems from six categories excluding geometry and 2 an experiment on the entire test set which includes 5000 problems.
We exclude AutoGPT from this evaluation as it cannot access results from code executions and does not solve any problems in the qualitative evaluation.
From these evaluations we have the following observations regarding the problem solving success rate and user experience of these systems.
Problem solving success rate Results from the quantitative evaluations show that AutoGen can help achieve the highest problem solving success rate among all the compared methods.
The qualitative evaluations elucidate common failure reasons across several alternative approaches.
ChatGPT Code Interpreter fails to solve the second problem and ChatGPT Plugin struggles to solve both problems.
AutoGPT fails on both problems due to code execution issues.
The LangChain agent also fails on both problems producing code that results in incorrect answers in all trials.
Based on the qualitative evaluation we analyze the user experience concerning the verbosity of the response and the ability of the LLM based system to run without unexpected behaviors.
ChatGPT Plugin is the least verbose mainly because Wolfram queries are much shorter than Python code.
AutoGen ChatGPT Code Interpreter and LangChain exhibit similar verbosity although LangChain is slightly more verbose due to more code execution errors.
AutoGPT is the most verbose system owing to predefined steps like THOUGHTS REASONING and PLAN which it includes in replies every time.
Overall AutoGen and ChatGPT Code Interpreter operate smoothly without exceptions.
We note the occurrences of undesired behaviors from other LLM based systems that could affect user experience AutoGPT consistently outputs code without the statement and cannot correct this requiring the user to run them manually ChatGPT with Wolfram Alpha plugin has the potential to become stuck in a loop that must be manually stopped and Langchain ReAct could exit with a parse error necessitating the passing of a parse parameter.
Figure 6 Examples of three settings utilized to solve math problems using AutoGen Gray Enables a workflow where a student collaborates with an assistant agent to solve problems either autonomously or in a human in the loop mode.
Facilitates a more sophisticated workflow wherein the assistant on the fly can engage another user termed who is in the loop with their own assistant agent to aid in problem solving if its own solutions are not satisfactory.
Scenario 2 Human in the loop Problem Solving.
Enable Multi User Problem Solving ViaStudent and Expert Student ProxyStudentAssistantExpertAssistantExpert ProxyAsk for expertEnable Autonomous and Human in the loop Problem Solving helpful.
To incorporate human feedback with AutoGen one can set human input mode in the user proxy agent.
We select one challenging problem that none of these systems can solve autonomously across three trials.
We adhere to the process outlined below to provide human inputs for all the compared methods.
1 Input the problem Find the equation of the plane which bisects the angle between the planes 3x 6y 2z 5 0 and 4x 12y 3z 3 0 and which contains the point.
Enter your answer in the form.
2 The response from the system does not solve the problem correctly.
We then give a hint to the model Your idea is not correct.
Suppose P x y z is a point that lies on a plane that bisects the angle the distance from P to the two planes is the same.
3 We expect the system to give the correct distance equation.
Since the equation involves an absolute sign that is hard to solve we would give the next hint Consider the two cases to remove the abs sign and get two possible solutions.
4 If the system returns the two possible solutions and continue to the next step we give the last hint Use point 5 1 5 to determine which is correct and give the final answer.
We observed that AutoGen consistently solved the problem across all three trials.
ChatGPT Code Interpreter and ChatGPT Plugin managed to solve the problem in two out of three trials while AutoGPT failed to solve it in all three attempts.
In its unsuccessful attempt ChatGPT Code Interpreter failed to adhere to human hints.
In its failed trial ChatGPT Plugin produced an almost correct solution but had a sign discrepancy in the final answer.
AutoGPT was unable to yield a correct solution in any of the trials.
In one trial it derived an incorrect distance equation.
In the other two trials the final answer was incorrect due to code execution errors.
Scenario 3 Multi User Problem Solving.
Next generation LLM applications may necessitate the involvement of multiple real users for collectively solving a problem with the assistance of LLMs.
We showcase how AutoGen can be leveraged to effortlessly construct such a system.
Specifically building upon scenario 2 mentioned above we aim to devise a simple system involving two human users a student and an expert.
In this setup the student interacts with an LLM assistant to address some problems and the LLM automatically resorts to the expert when necessary.
The overall workflow is as follows The student chats with the LLM based assistant agent through a student proxy agent to solve problems.
When the assistant cannot solve the problem satisfactorily or the solution does not match the expectation of the student it would automatically hold the conversation and call the pre defined ask for expert function via the function call feature of GPT in order to resort to the expert.
Specifically it would automatically produce the initial message for the ask for expert function which could be the statement of the problem or the request to verify the solution to a problem and the expert is supposed to respond to this message with the help of the expert assistant.
After the conversation between the expert and the assistant the final message would be sent back to the student assistant as the response to the initial message.
Then the student assistant would resume the conversation with the student using the response from the expert for a better solution.
With AutoGen constructing the student expert proxy agent and the assistant agents is straightforward by reusing the built in UserProxyAgent and AssistantAgent through appropriate configurations.
The only development required involves writing several lines of code for the ask for expert function which then becomes part of the configuration for the assistant.
Additionally easy to extend such a system to include more than one expert with a specific ask for expert function for each or to include multiple student users with a shared expert for consultation.
A2 Retrieval Augmented Code Generation and Question Answering.
Figure 7 Overview of Retrieval augmented Chat which involves two agents including a Retrievalaugmented User Proxy and a Retrieval augmented Assistant.
Given a set of documents the Retrieval augmented User Proxy first automatically processes chunks and stores them in a vector database.
Then for a given user input it retrieves relevant chunks as context and sends it to the Retrieval augmented Assistant which uses LLM to generate code or text to answer questions.
Agents converse until they find a satisfactory answer.
To use Retrieval augmented Chat one needs to initialize two agents including Retrieval augmented User Proxy and Retrieval augmented Assistant.
Initializing the Retrieval Augmented User Proxy necessitates specifying a path to the document collection.
Subsequently the Retrieval Augmented User Proxy can download the documents segment them into chunks of a specific size compute embeddings and store them in a vector database.
Once a chat is initiated the agents collaboratively engage in code generation or question answering adhering to the procedures outlined below.
1 The Retrieval Augmented User Proxy retrieves document chunks based on the embedding simi larity and sends them along with the question to the Retrieval Augmented Assistant.
2 The Retrieval Augmented Assistant employs an LLM to generate code or text as answers based on the question and context provided.
If the LLM is unable to produce a satisfactory response it is instructed to reply with to the Retrieval Augmented User Proxy.
3 If a response includes code blocks the Retrieval Augmented User Proxy executes the code and sends the output as feedback.
If there are no code blocks or instructions to update the context it terminates the conversation.
Otherwise it updates the context and forwards the question along with the new context to the Retrieval Augmented Assistant.
Note that if human input solicitation is enabled individuals can proactively send any feedback including Update to the Retrieval Augmented Assistant.
4 If the Retrieval Augmented Assistant receives it requests the next most similar chunks of documents as new context from the Retrieval Augmented User Proxy.
Otherwise it generates new code or text based on the feedback and chat history.
If the LLM fails to generate an answer it replies with again.
This process can be repeated several times.
The conversation terminates if no more documents are available for the context.
We utilize Retrieval Augmented Chat in two scenarios.
The first scenario aids in generating code based on a given codebase.
While LLMs possess strong coding abilities they are unable to utilize packages or APIs that are not included in their training data e g private codebases or have trouble using trained ones that are frequently updated post training.
Hence Retrieval Augmented Code Generation is considered to be highly valuable.
The second scenario involves question answering on the Natural Questions dataset Kwiatkowski et al 2019 enabling us to obtain comparative evaluation metrics for the performance of our system.
Scenario 1 Evaluation on Natural Questions QA dataset.
In this case we evaluate the Retrieval Augmented end to end question answering performance using the Natural Questions dataset Kwiatkowski et al 2019.
We collected 5 332 non redundant context documents and 6 775 queries from HuggingFace.
First we create a document collection based on the entire context corpus and store it in the vector database.
Then we utilize Retrieval Augmented Chat to answer the questions.
An example Figure 8 from the NQ dataset showcases the advantages of the interactive retrieval feature carried the usa flag in opening.
Retrieval augmentedAssistantRetrieval augmented User Proxy1.
Satisfied Answers or Figure 8 Retrieval augmented Chat without W O and with W interactive retrieval.
As a result the LLM assistant GPT 3 5 turbo replies I cannot find any information about who carried the USA flag in the opening ceremony UPDATE CONTEXT.
With the unique and innovative ability to update context in Retrieval Augmented Chat the user proxy agent automatically updates the context and forwards it to the assistant agent again.
Following this process the agent is able to generate the correct answer to the question.
In addition we conduct an experiment using the same prompt as illustrated in Adlakha et al 2023 to investigate the advantages of AutoGen W O interactive retrieval.
Consequently we assert that AutoGen W O interactive retrieval outperforms DPR due to differences in the retrievers employed.
Specifically we utilize a straightforward vector search retriever with the all MiniLM L6 v2 model for embeddings.
Furthermore we analyze the number of LLM calls in experiments involving both AutoGen and AutoGen W O interactive retrieval revealing that approximately 19 4 % of questions in the Natural Questions dataset trigger an operation resulting in additional LLM calls.
Scenario 2 Code Generation Leveraging Latest APIs from the Codebase.
In this case the question is can I use FLAML to perform a classification task and use Spark for parallel training Train for 30 seconds and force cancel jobs if the time limit is reached.
FLAML v1 Wang et al 2021 is an open source Python library designed for efficient AutoML and tuning.
However the question necessitates the use of Spark related APIs which were added in December 2022 and are not encompassed in the GPT 4 training data.
Consequently the original GPT 4 model is unable to generate the correct code due to its lack of knowledge regarding Spark related APIs.
Nevertheless with Retrieval Augmented Chat we provide the latest reference documents as context.
Task description who carried the usaflag in opening ceremony.
Assistant a retrieve augmented chatbot.
You answer questions based on your own knowledge and the context provided by the user.
If you answer the question with or without the current context you should reply exactly.
You must give as short an answer as possible question is who carried the usaflag in opening ceremonyContext is P On January 17 1899 under.
P Sorry I cannot find any information about who carried the USA flag in the opening ceremony.
UPDATE CONTEXT Assistant a retrieve augmented chatbot.
You answer questions based on your own knowledge and the context provided by the user.
If you answer the question with or without the current context you should reply exactly.
You must give as short an answer as possible question is who carried the usaflag in opening ceremonyContext is T able.
Erin Hamlin T d Erin Hamlin carried the USA flag in the opening ceremony.
a W O Interactive Retrieval.
b W Interactive RetrievalAssistant a retrieve augmented chatbot.
You answer questions based on your own knowledge and the context provided by the user.
You must give as short an answer as possible question is who carried the usaflag in opening ceremonyContext is P On January 17 1899 under.
P Sorry I cannot find any information about who carried the USA flag in the opening ceremony.
A3 Decision Making in Text World Environments.
Figure 9 We use AutoGen to solve tasks in the ALFWorld benchmark which contains household tasks described in natural language.
We propose two designs a two agent design where the assistant agent suggests the next step and the Executor executes actions and provides feedback.
The threeagent design adds a grounding agent that supplies commonsense facts to the executor when needed.
ALFWorld Shridhar et al 2021 is a synthetic language based interactive decision making task.
It comprises textual environments that aim to simulate real world household scenes.
Given a highlevel goal e g putting a hot apple in the fridge and the description of the household environment the agent needs to explore and interact with the simulated household environment through a textual interface.
A typical task environment contains various types of locations and could require more than 40 steps to finish which highlights the need for agents to decompose the goal into subtasks and tackle them one by one while effectively exploring the environments.
We first propose a straightforward two agent system with AutoGen illustrated on the left hand side of Figure 9 to tackle tasks from this benchmark.
The system consists of an assistant agent and an executor agent.
The assistant agent generates plans and makes action decisions to solve the tasks.
The executor agent is tailored specifically for ALFWorld.
It performs actions proposed by the assistant and reports action execution results in the household environment as feedback to the assistant.
Due to the strict format requirements for the output format we use the BLEU metric to evaluate the similarity of the output to all valid action options.
The option with the highest similarity will be chosen as the action for this round.
One major challenge encompassed in ALFWorld is commonsense reasoning.
The agent needs to extract patterns from the few shot examples provided and combine them with the general knowledge of household environments to fully understand task rules.
More often than not the assistant tends to neglect some basic knowledge of the household environment.
Thanks to the easy toimplement multi agent conversational feature of AutoGen enhancing the assistant reasoning ability by adding a new grounding agent to provide commonsense facts for the decision making reference becomes straightforward.
By scrutinizing the failed attempts and summarizing the reasons for failure we obtained a holistic understanding of the commonsense knowledge that the assistant agent lacks.
This ensures the assistant takes this commonsense knowledge into consideration and prevents it from getting stuck in outputting the same content or constantly apologizing.
We compare our performance with ReAct which treats ALFWorld as a text completion task.
ReAct Yao et al 2022 is a few shot prompting technique that interleaves reasoning and acting allowing for greater synergy between the two and significantly improving performance on both language and decision making tasks.
We integrate ReAct into AutoGen by modifying the prompts in a conversational manner.
Following ReAct we employ a two shot setting.
The fewshot prompts are obtained from the corresponding repository.
As shown in Table 3 the two agent.
ALFWorldExecutorReward StateAction DecisionAssistantObservation On the desk 2 you see an alarmclock3 a bowl 3 a creditcard2 a mug 1 and a pencil 2 Action decision Pick up pencil 2 from desk 2GroundingAgentALFChat two agents ALFChat three agents ALFWorldExecutorAssistant Figure 10 Comparison of results from two designs.
a Two agent design which consists of an assistant and an executor.
b Three agent design which adds a grounding agent that serves as a knowledge source.
For simplicity we omit the in context examples and part of the exploration trajectory and only show parts contributing to the failure success of the attempt.
Pick Clean Heat Cool Look.
Table 3 Comparisons between ReAct and the two variants of ALFChat on the ALFWorld benchmark.
For each task we report the success rate out of 3 attempts.
Success rate denotes the number of tasks successfully completed by the agent divided by the total number of tasks.
The results show that adding a grounding agent significantly improves the task success rate in ALFChat.
design matches the performance of ReAct while the three agent design significantly outperforms ReAct.
We surmise that the performance discrepancy is caused by the inherent difference between dialogue completion and text completion tasks.
On the other hand introducing a grounding agent as a knowledge source remarkably advances performance on all types of tasks.
Figure 10 exemplifies how a three agent design eliminates one root cause for failure cases.
Most of the tasks involve taking an object and then performing a specific action with it e g finding a vase and placing it on a cupboard.
This leads to most of the failure cases in and type tasks.
With the introduction of a grounding agent the assistant can break out of this loop and successfully complete the task.
We introduced a grounding agent to serve as an external commonsense knowledge source which significantly enhanced the ability to make informed decisions.
This proves that providing necessary commonsense facts to the decision making agent can assist it in making more informed decisions thus effectively boosting the task success rate.
AutoGen brings both simplicity and modularity when adding the grounding agent.
Task descriptionYou are in the middle of a room.
Looking quickly around you you see a bed 1 a desk 2 a desk 1 a safe 1 a drawer 2 a drawer1 a shelf 3 a shelf 2 and a shelf 1 Your task is to look at bowl under the desklamp.
Assistant ACTION use desklamp1 Executor to Assistant You turn on the desklamp1 Assistant to Executor THOUGHT Now that the desklampis turned on I can use it to look at the bowl 1 that I found earlier ACTION look at bowl 1 under the desklamp1 Executor to Assistant You turn on the desklamp1 Assistant falls into an infinite loop turning on desklamp Executor to Assistant Task failed.
You must go to where the target object is before you can use it Executor to Assistant You turn on the desklamp1.
Assistant to Executor ACTION take bowl 1 from desk 2 Executor to Assistant You pick up bowl 1 from desk 2 Assistant to Executor ACTION go to desk 1 Assistant to Executor ACTION use desklamp1 Executor to Assistant Task success.
Reply TERMINATE Assistant to Executor TERMINATE.
b Three agents A4 Multi Agent Coding.
Figure 11 Our re implementation of OptiGuide with AutoGen streamlining interactions.
The Commander receives user questions e g What if we prohibit shipping from supplier 1 to roastery 2 and coordinates with the Writer and Safeguard.
The Writer crafts the code and interpretation the Safeguard ensures safety e g not leaking information no malicious code and the Commander executes the code.
If issues arise the process can repeat until resolved.
Shaded circles represent steps that may be repeated multiple times.
The workflow can be described as follows.
The end user initiates the interaction by posing a question such as if we prohibit shipping from supplier 1 to roastery 2 marked by to the Commander agent.
The Commander manages and coordinates with two LLM based assistant agents the Writer and the Safeguard.
Apart from directing the flow of communication the Commander has the responsibility of handling memory tied to user interactions.
This capability enables the Commander to capture and retain valuable context regarding the questions and their corresponding responses.
Such memory is subsequently shared across the system empowering the other agents with context from prior user interactions and ensuring more informed and relevant responses.
In this orchestrated process the Writer who combines the functions of a and an as defined in Li et al 2023a will craft code and also interpret execution output logs.
For instance during code writing the Writer may craft code addConstr x 0 to add an additional constraint to answer the question.
After receiving the code the Commander will communicate with the Safeguard to screen the code and ascertain its safety the Commander will use external tools e g Python to execute the code and request the Writer to interpret the execution results for the question.
Bringing this intricate process full circle the Commander furnishes the user with the concluding answer.
If at a point there is an exception either a security red flag raised by Safeguard in or code execution failures within Commander the Commander redirects the issue back to the Writer with essential information in logs might be repeated multiple times until each user query receives a thorough and satisfactory resolution or until the timeout.
This entire complex workflow of multi agent interaction is elegantly managed via AutoGen.
The core workflow code for OptiGuide was reduced from over 430 lines to 100 lines using AutoGen leading to significant productivity improvement.
The new agents are customizable conversable and can autonomously manage their chat memories.
This consolidation allows the coder and interpreter roles to merge into a single agent resulting in a clean concise and intuitive implementation that is easier to maintain.
CommanderSafeguardWriter②Question ❻Log❸Code ⑦Ans❹Code❺ClearanceUser①User Question⑧Final AnswerRepeat until answering the question or timeout123456785636 Manual Evaluation Comparing ChatGPT Code Interpreter and AutoGen based OptiGuide.
ChatGPT Code Interpreter is unable to execute code with private or customized dependencies e g Gurobi which means users need to have engineering expertise to manually handle multiple steps disrupting the workflow and increasing the chance for mistakes.
If users lack access or expertise the burden falls on supporting engineers increasing their on call time.
We carried out a user study that juxtaposed ChatGPT coupled with a Code Interpreter against AutoGen based OptiGuide.
The study focused on a coffee supply chain scenario and an expert Python programmer with proficiency in Gurobi participated in the test.
We evaluated both systems based on 10 randomly selected questions measuring time and accuracy.
While both systems answered 8 questions correctly the Code Interpreter was significantly slower than OptiGuide because the former requires more manual intervention.
On average users needed to spend 4 minutes and 35 seconds to solve problems with the Code Interpreter with a standard deviation of approximately 2 5 minutes.
In contrast average problem solving time was around 1 5 minutes most of which was spent waiting for responses from the GPT 4 model.
This indicates a 3x saving on the time with AutoGen based OptiGuide.
While using ChatGPT Code Interpreter users had to read through the code and instructions to know where to paste the code snippets.
Additionally running the code involves downloading it and executing it in a terminal a process that was both time consuming and prone to errors.
The response time from the Code Interpreter is also slower as it generates lots of tokens to read the code read the variables line by line perform chains of thought analysis and then produce the final answer code.
In contrast AutoGen integrates multiple agents to reduce user interactions by 3 5 times on average as reported in Table 4 where we evaluated our system with 2000 questions across five OptiGuide applications and measured how many prompts the user needs to type.
Table 4 Manual effort saved with OptiGuide W GPT 4 while preserving the same coding performance is shown in the data below.
The data include both the mean and standard deviations indicated in parentheses.
Table 13 and 15 provide a detailed comparison of user experience with ChatGPT Code Interpreter and AutoGen based OptiGuide.
ChatGPT Code Interpreter is unable to run code with private packages or customized dependencies such as Gurobi as a consequence ChatGPT Code Interpreter requires users to have engineering expertise and to manually handle multiple steps disrupting the workflow and increasing the chance for mistakes.
If customers lack access or expertise the burden falls on supporting engineers increasing their on call time.
In contrast the automated chat by AutoGen is more streamlined and autonomous integrating multiple agents to solve problems and address concerns.
This results in a 5x reduction in interaction and fundamentally changes the overall usability of the system.
Takeaways The implementation of the multi agent design with AutoGen in the OptiGuide application offers several advantages.
It simplifies the Python implementation and fosters a mixture of collaborative and adversarial problem solving environments with the Commander and Writer working together while the Safeguard acts as a virtual adversarial checker.
This setup allows for proper memory management as the Commander maintains memory related to user interactions providing context aware decision making.
Additionally role playing ensures that each memory remains isolated preventing shortcuts and hallucinations.
Figure 12 A5 Dynamic Group Chat Overview of how AutoGen enables dynamic group chats to solve tasks.
The Manager agent which is an instance of the GroupChatManager class performs the following three a single speaker in this case Bob ask the speaker to respond and broadcast the selected message to all other agents.
To validate the necessity of multi agent dynamic group chat and the effectiveness of the role play speaker selection policy we conducted a pilot study comparing a four agent dynamic group chat system with two possible alternatives across 12 manually crafted complex tasks.
An example task is much money would I earn if I bought 200 $ AAPL stocks at the lowest price in the last 30 days and sold them at the highest price Save the results into a file.
The four agent group chat system comprised the following group members a user proxy to take human inputs an engineer to write code and fix bugs a critic to review code and provide feedback and a code executor for executing code.
One of the possible alternatives is a two agent system involving an LLM based assistant and a user proxy agent and another alternative is a group chat system with the same group members but a task based speaker selection policy.
In the task based speaker selection policy we simply append role information chat history and the next task into a single prompt.
Table 5 Number of successes on the 12 tasks higher the better.
Two Agent Group Chat Group Chat with a task based speaker selection policy.
Table 6 Average LLM calls and number of termination failures on the 12 tasks lower the better.
Two Agent Group Chat Group Chat with a task based speaker selection policy.
Select a Speaker AliceBobUser ProxyBob2.
Ask the Speaker to RespondManagerManagerResponse Figure 13 Comparison of two agent chat.
b on a given task.
The group chat resolves the task successfully with a smoother conversation while the two agent chat fails on the same task and ends with a repeated conversation.
Here the board agent maintains the rules of the game and supports the players with information about the board.
Players and the board agent all use natural language for communication.
A third party known as the board agent is designed to provide players with information about the board and ensure that moves adhere to legal chess moves.
Figure 14 illustrates the scenarios supported by Conversational Chess AI human vs AI human and demonstrates how players and the board agent interact.
This setup fosters social interaction and allows players to express their moves creatively employing jokes meme references and character playing thereby making chess games more entertaining for both players and observers Figure 15 provides an example of conversational chess.
To realize these scenarios we constructed a player agent with LLM and human as back end options.
When human input is enabled before sending the input to the board agent it first prompts the human player to input the message that contains the move along with anything else the player wants to say such as a witty comment.
If human input is skipped or disabled LLM is used to generate the message.
The board agent is implemented with a custom reply function which employs an LLM to parse the natural language input into a legal move in a structured format e g UCI and then pushes the move to the board.
If the move is not legitimate the board agent will reply with an error.
Subsequently the player agent needs to resend a message to the board agent until a legal move is made.
Once the move is successfully pushed the player agent sends the message to the opponent.
As shown in Figure 15 the conversation between AI players can be natural and entertaining.
When the player agent uses LLM to generate a message it utilizes the board state and the error message from the board agent.
This helps reduce the chance of hallucinating an invalid move.
The chat between one player agent and the board agent is invisible to the other player agent which helps keep the messages used in chat completion well managed.
There are two notable benefits of using AutoGen to implement Conversational Chess.
Firstly the agent design in AutoGen facilitates the natural creation of objects and their interactions needed in our chess game.
This makes development easy and intuitive.
For example the isolation of chat messages simplifies the process of making a proper LLM chat completion inference call.
Secondly AutoGen greatly simplifies the implementation of agent behaviors using composition.
Specifically we utilized the register reply method supported by AutoGen agents to instantiate player agents and a board agent with custom reply functions.
Concentrating the extension work needed at a single point the reply function simplifies the reasoning processes and development and maintenance effort.
Chess BoardHuman AI Chess Player AHuman AI Chess Player BValidate moveValidate moveChallenging your pawn in the center.
Your move Developing my knightto a good square Your move.
a Conversation between two AI players.
b Conversation between the AI players player white shown in blue and the board agent for making a new move.
Figure 15 Example conversations during a game involving two AI player agents and a board agent.
In this demonstration instead of employing an additional board agent for grounding the system utilizes prompting for grounding by including the sentence should make sure both you and the opponent are making legal moves in the system messages directed to both players.
b with a board Conversational Chess.
Ah the King s Pawn Opening a classic.
I ll move my pawn from e7 to e5 You know I ve always admired how chess reflects life.
chess is a mirror of life in many ways.
Every move matters just like every decision in life.
I ll move my knight from g1 to f3 aiming to control the center Error illegal uci e4e5 in I ll move my pawn from d2 to d4 opening uplines for my queen and bishop Indeed the king s safety is paramount in the early game.
I ll move my pawn from e4 to e5 attacking your knight Player White Your movePlayer black Alright let s start.
P P P P P P P PR N B Q K B N RYour turn.
Here s the updated board.
You re giving me a tough time here Player White Your movePlayer black Alright let s start.
P P P P P P P PR N B Q K B N RYour turn.
Here s the updated board.
RConversational ChessSystem message to the player agents Your name is name and you are a chess player.
You are playing as color.
You communicate your move using universal chess interface language.
You also chit chat with your opponent when you communicate a move to light up the mood You should make sure both you and the opponent are making legal moves.
a W O Board Agent A7 Online Decision Making for Browser interactions.
Figure 17 We use AutoGen to build MiniWobChat which solves tasks in the MiniWob benchmark.
MiniWobChat consists of two agents an assistant agent and an executor agent.
The assistant agent suggests actions to manipulate the browser while the executor executes the suggested actions and returns rewards feedback.
The assistant agent records the feedback and continues until the feedback indicates task success or failure.
In practice many applications require the presence of agents capable of interacting with environments and making decisions in an online context such as in game playing Mnih et al 2013 Vinyals et al 2017 web interactions Liu et al 2018 Shi et al 2017 and robot manipulations Shen et al 2021.
With the multi agent conversational framework in AutoGen it becomes easy to decompose the automatic agent environment interactions and the development of a decision making agent by constructing an executor agent responsible for handling the interaction with the environment thereby delegating the decision making part to other agents.
Such a decomposition allows developers to reuse the decision making agent for new tasks with minimal effort rather than building a specialized decision making agent for every new environment.
We demonstrate how to use AutoGen to build a working system for handling such scenarios with the MiniWoB benchmark Shi et al 2017.
MiniWoB comprises browser interaction tasks that involve utilizing mouse and keyboard actions to interact with browsers.
The ultimate objective of each task is to complete the tasks described concisely in natural language such as the web section below and click the submit button.
Solving these tasks typically requires a sequence of web manipulation actions rather than a single action and making action decisions at each time step requires access to the web status in the form of HTML code online.
For the example above clicking the submit button requires checking the web status after expanding the web section.
The assistant agent is an instance of the built in AssistantAgent and is responsible for making action decisions for the given task.
The second agent the executor agent is a customized UserProxyAgent which is responsible for interacting with the benchmark by executing the actions suggested by the AssistantAgent and returning feedback.
To assess the performance of the developed working system we compare it with RCI Kim et al 2023 a recent solution for the MiniWoB benchmark that employs a set of self critiquing prompts and has achieved state of the art performance.
In our evaluation we use all available tasks in the official RCI code with varying degrees of difficulty to conduct a comprehensive analysis against MiniWobChat.
Figure 18 illustrates that MiniWobChat achieves competitive performance in this evaluation8.
Specifically among the 49 available tasks MiniWobChat achieves a success rate of 52 8 % which is only 3 6 % lower than RCI a method specifically designed for the MiniWob benchmark.
8We report the results of RCI by running its official code with default settings.
ExecutorReward StateAction DecisionAssistantEnvironment State HTML code for current web pagesReward Success Fail OngoingAction decision Next action to perform on a web pageAction decision the button with button id State id wrap data 2 data e0 id query button ONE then click button TWO id area data 3 data e0 id subbtn style position absolute left 50px top 74px data 4 data e0 id subbtn2 style position absolute left 98px top 167px data 5 data e0 Ongoing other on the same number of tasks.
For illustration purposes we provide a case analysis in Table 7 on four typical tasks.
Additionally we also explored the feasibility of using Auto GPT for handling the same tasks.
AutoGPT faces challenges in handling tasks that involve complex rules due to its limited extensibility.
It provides an interface for setting task goals using natural language.
However when dealing with the MiniWob benchmark accurately instructing Auto GPT to follow the instructions for using MiniWob proves challenging.
There is no clear path to extend it in the manner of the two agent chat facilitated by AutoGen.
Takeaways For this application AutoGen stood out as a more user friendly option offering modularity and programmability It streamlined the process with autonomous conversations between the assistant and executor and provided readily available solutions for agent environment interactions.
The built in AssistantAgent was directly reusable and exhibited strong performance without customization.
Moreover the decoupling of the execution and assistant agent ensures that modifications to one component do not adversely impact the other.
This convenience simplifies maintenance and future updates.
Figure 18 Comparisons between RCI state of the art prior work and MiniWobChat on the MiniWob benchmark are elucidated herein.
We utilize all available tasks in the official RCI code each with varying degrees of difficulty to conduct comprehensive comparisons.
For each task the success rate across ten different instances is reported.
When a success rate tolerance of 0 1 is considered for each task both methods outperform each other on an equal number of tasks.
Table 7 Cases analysis on four typical tasks from MiniWob.
AssistantAgent provides actions with infeasible characters.
RCI performs actions that are out of its plan.
AssistantAgent provide actions with redundant content that can not convert to actions in the benchmark.
RCI provides a wrong plan in most cases.
AssistantAgent return actions out of its plan.
RCI provides a wrong plan in most cases.
choose listclick button sequenceclick buttonclick checkboxes largeclick checkboxes softclick checkboxes transferclick checkboxesclick collapsible 2click collapsibleclick colorclick dialog 2click dialogclick linkclick menuclick optionclick scroll listclick shadesclick shapeclick tab 2 hardclick tab 2click tabclick test 2click testclick widgetcount shapeemail inbox forward nl turkemail inbox forward nlemail inbox nl turkemail inboxenter dateenter passwordenter text dynamicenter textenter timefocus text 2focus textgrid coordinatelogin user popuplogin usernavigate treesearch enginesimple algebrasocial media allsocial media somesocial mediaterminaluse spinner0 00 51 0success rateRCIMiniWobChat E Example outputs from applications.
In this section we include example outputs from the following applications and systems.
Application A1 preliminary evaluation on several alternative multi agent systems BabyAGI.
Table 8 Application A1 Scenario 1 Autonomous math problem solving.
ChatGPT Plugin Wolfram Alpha on the first qualitative example.
Completely simplify and rationalize the denominator.
2 0 0 1 3 7 1 6 6 3 7 1 8 2 5 9 6 8 6 9 7 4 0 1 3 7 7 0 5 3 3 3 2 7 1 4 3 8 9 8 5 2 6 4 9 4 5 5 2 15 Wolfram Alpha website result for simplify 16 sqrt 160 sqrt 252 sqrt 245 sqrt 108 17 link.
5 cid 112 14 3 In decimal form this is approximately 1 200137166371825968697401377053332714.
Table 9 Application A1 Scenario 1 Autonomous math problem solving.
AutoGen on the first qualitative example.
AutoGen solves the problem correctly.
Completely simplify and rationalize the denominator.
We can simplify and rationalize this denominator using symbolic mathematics library.
Here is the Python code to resolve this.
1 from sympy import sqrt Rational 3 Define the entire fraction as a rational number 4 fraction sqrt 160 sqrt 252 sqrt 245 sqrt 108 6 Simplify the fraction 7 simplified fraction.
simplify 9 Print the simplified fraction 10 print simplified.
This code performs the requested simplification by constructing the fraction using the function from the module which automatically simplifies fractions when possible.
The function from the module enables square root calculations.
Finally the function simplifies expressions when possible.
Table 10 Application A1 Scenario 1 Autonomous math problem solving.
LangChain ReAct on the first qualitative example.
The generated code does not match the devised plan.
The code returns a decimal number.
Completely simplify and rationalize the denominator.
To simplify this expression I will first simplify each square root individually then multiply the fractions as normal.
The result should be a rational number.
Action Python REPL Action Input.
1 import math 3 Calculate square roots 4 math.
Observation 1 200137166371826 Thought I now know the final answer Final Answer The simplified and rationalized form of the expression is approximately 1 200137166371826.
Table 11 Application A1 Scenario 1 Autonomous math problem solving.
AutoGPT on the first qualitative example.
We abort as the code never prints the result and the system does not rectify it after 2 turns.
AutoGPT is much more verbose compared to other frameworks.
We have shortened some unnecessary content.
Would you like me to return to being MathSolverGPT.
Continue with the last settings.
Name MathSolverGPT Role an intelligent math solving AI that assists students professionals.
Goals step by step solutions to math problems ensuring clarity and understanding for the user.
MathSolverGPT has been created with the following details Name MathSolverGPT Role an intelligent math solving AI that assists students professionals and enthusiasts in solving a wide range of math problems from basic arithmetic to advanced calculus and beyond.
Goals Provide step by step solutions to math problems ensuring clarity and understanding for the user.
Offer explanations and insights into the underlying concepts and principles behind the solutions fostering a deeper understanding of mathematics.
Handle a variety of math problem types including algebra geometry trigonometry calculus and more.
The user provided any context or problem to solve.
REASONING The request is too vague.
They specified what they want me to do or what problem they want me to solve.
PLAN Ask the user for more information or a specific task.
CRITICISM I should have asked the user for more information or a specific task in the first place.
SPEAK Could you please provide more information or specify a task.
Enter to authorise command to run N continuous commands to exit program or enter feedback for MathSolverGPT.
Completely simplify and rationalize the denominator.
MATHSOLVERGPT THOUGHTS To solve this problem I need to simplify and rationalize the denominator of the given expression.
This involves simplifying the square roots and then multiplying the fractions.
I can use math library to calculate the square roots and perform the multiplication.
REASONING The problem involves simplifying square roots and multiplying fractions.
math library provides functions for these operations.
PLAN Write a Python script to calculate the square roots and multiply the fractions.
Execute the script to get the simplified expression.
CRITICISM I need to ensure that I correctly calculate the square roots and perform the multiplication.
I should also make sure to rationalize the denominator as the problem specifies.
SPEAK simplify and rationalize the denominator of the given expression.
NEXT ACTION COMMAND execute python code ARGUMENTS.
COMMAND AUTHORISED BY USER Executing python file app autogpt workspa.
SYSTEM Command execute python code returned Error Traceback most recent call last File line 13 in simplified result math simplify result AttributeError module has no attribute.
MATHSOLVERGPT THOUGHTS The error message indicates that the module in Python does not have a function.
I need to use the library instead which provides a function for simplifying expressions.
REASONING The module in Python does not provide a function for simplifying expressions.
The library on the other hand does provide such a function.
PLAN Rewrite the Python script to use the library instead of the module.
Execute the script to get the simplified expression.
CRITICISM I should have known that the module does not provide a function for simplifying expressions.
I should have used the library from the start.
SPEAK try again using the library to simplify the expression.
NEXT ACTION COMMAND execute python code ARGUMENTS.
SYSTEM Command execute python code returned.
MATHSOLVERGPT THOUGHTS The command execution return any output which is unusual.
It seems there might be an issue with the execution of the Python code.
I need to debug this.
REASONING The Python code execution should have returned a result.
The absence of a result suggests that there might be an issue with the code or its execution.
PLAN Review the Python code to identify any potential issues.
If necessary modify the code and execute it again.
SPEAK It seems there was an issue with the execution of the Python code.
NEXT ACTION COMMAND execute python code ARGUMENTS Exact same code as last.
SYSTEM Command execute python code returned.
MATHSOLVERGPT THOUGHTS The Python code execution still returning any output.
possible that the issue lies with the calculation of the square roots.
Table 12 Application A1 Scenario 1 Autonomous math problem solving.
Multi Agent Debate on the first qualitative problem.
It gives a wrong final answer.
Completely simplify and rationalize the denominator.
Debate Round 1 Affirmative side Ok simplify each square root separately.
160 160 can be decomposed as 16 10 where 16 is a perfect square so 252 252 can be decomposed as 36 7 where 36 is a perfect square so 245 108 108 can be decomposed as 36 3 where 36 is a perfect square so Then we substitute the simplified square roots back into the expression we have.
We are almost done but we still need to rationalize the square root part of expression.
multiply the square root part by at both the top and bottom to get rid of the square root at the denominator.
Then this is the final answer.
can be simplified to 7 Negative side I do disagree with your final answer but I think we mostly agree on the steps.
Your simplifications of the square roots are correct but you made a mistake in the final simplification part.
Let me explain how I would complete the problem As stated.
now see if we disagree on this final answer.
Moderator there is a sides of the debate came to the same final answer However the negative side correctly identified a mistake in the affirmative calculation pointing out that the simplification should have been the square root of 50 over the square root of 21 not 50 over 21 The negative side shows a better understanding of the simplification process 7 Debate Done.
Base Answer Ok simplify each square root separately.
160 160 can be decomposed as 16 10 where 16 is a perfect square so 252 252 can be decomposed as 36 7 where 36 is a perfect square so 245 108 108 can be decomposed as 36 3 where 36 is a perfect square so Then we substitute the simplified square roots back into the expression we have.
We are almost done but we still need to rationalize the square root part of expression.
multiply the square root part by at both the top and bottom to get rid of the square root at the denominator.
Then this is the final answer.
Debate Reason Both sides of the debate came to the same final answer.
The negative side shows a better understanding of the simplification process.
ChatGPT Code Interpreter for OptiGuide.
A sample question if the roasting cost is increased by 5 % because of the potential salary increase is asked.
For the source code Can you tell me What if the roasting cost is increased by 5 % because of the potential salary increase.
first take a look at the contents of the file to understand the code and how the roasting cost is calculated.
This will allow us to determine the effect of a 5 % increase in the roasting cost.
several lines omitted Would you like me to modify the code to reflect the 5 % increase in roasting costs and run the modified code to observe the impact.
If the code requires any specific input or parameters please let me know.
Before proceeding need to ensure that the Gurobi solver is not being used in this code as my current environment does not have access to Gurobi.
If the code does indeed use Gurobi we can still analyze the effect of the change in roasting costs by understanding the structure of the code.
several lines omitted Alternatively if you have access to a Gurobi license you can make the necessary changes in the code and run it on your local machine.
I can guide you through the process of making the necessary modifications to the code.
Would you like me to proceed with the code analysis and explanation or would you like guidance on making the changes yourself.
You can give me the full code and I can run the code locally myself.
modify the code to reflect a 5 % increase in both the light and dark roasting costs and then provide the full code for you to run locally.
I have tried to run your code.
Hint change roasting cost light or roasting cost light variables.
I apologize for the oversight.
It appears I made direct changes to the variables that could have led to issues with the code.
take a different approach by analyzing the code to identify the appropriate place to apply the 5 % increase in roasting costs.
Hi GPT can you safecheck the generated code for me one more time.
I have tried to run your code again.
Hint change roasting cost light or roasting cost light variables.
I apologize for the continued confusion.
Hi GPT can you safecheck the generated code for me one more time.
Here is the execution result.
Remember to compare with the original cost of 2470.
the comparison Original Cost 2470 Modified Cost with 5 % increase in roasting costs 2526 5 The increase in roasting costs has led to an increase in the overall cost by 56 5 reflecting the impact of the potential salary increase on the roasting cost.
If you need any further insights or assistance please let me know.
Scenario 1 autonomous math solving.
ChatGPT Code Interpreter on the first qualitative example.
The code generated does not match the devised plan.
The code returns a decimal number.
Completely simplify and rationalize the denominator.
To simplify and rationalize the expression we first simplify the radicals in the numerator and denominator by factoring out the largest perfect squares from the numbers under the square root.
The perfect square factors of 160 252 245 and 108 are 16 4 1 and 36 respectively.
We can simplify this further by applying the property Python.
1 import math 3 factoring the square roots 4 math.
The same question if the roasting cost is increased by 5 % because of the potential salary increase is asked.
g Prompt What if the roasting cost is increased by 5 % because of the potential salary increase.
After increasing the roasting costs by 5 % re calculated the optimal coffee distribution solution for the supply chain.
This indicates that the increased roasting costs have caused the overall distribution cost to rise by 56 5 units.
Scenario 1 autonomous math solving.
OBJECTIVE Solve math problems INITIAL TASK Completely simplify and rationalize the denominator.
Initial task Completely simplify and rationalize the denominator.
TASK LIST Completely simplify and rationalize the denominator.
NEXT TASK Completely simplify and rationalize the denominator.
TASK RESULT First simplify each square root separately.
Now substitute these values into the original expression which gives.
We get rid of the square root in the denominator by multiplying the fractions which gives.
20 20 Simplifying further gives 20 TASK CREATION AGENT PROMPT You are to use the result from an execution agent to create new tasks with the following objective Solve math problems.
The last completed task has the result First simplify each square root separately.
Now substitute these values into the original expression which gives.
We get rid of the square root in the denominator by multiplying the fractions which gives.
This result was based on this task description Completely simplify and rationalize the denominator.
Simplifying further gives 20 Based on the result return a list of tasks to be completed in order to meet the objective.
Return one task per line in your response.
The result must be a numbered list in the format First task Second task The number of each entry must be followed by a period.
If your list is empty write are no tasks to add at this time.
Unless your list is empty do not include any headers before your numbered list or follow your numbered list with any other output.
Simplify and rationalize the denominator for the expression.
Adding new tasks to task storage 2 and rationalize the denominator for the expression 3 Until task id 11 TASK PRIORITIZATION AGENT PROMPT tasked with prioritizing the You are pression fracsqrt225sqrt45timesfracsqrt200sqrt125 Simplify and rationalize the denominator sqrt289sqrt361timesfracsqrt100sqrt72 Aborted.
exfor the expression frac the denominator.
Table 17 Application A1 Scenario 1 Autonomous math problem solving.
The resulting code cannot be executed.
The conversation continues and the roles are not keen on solving the problem and giving an answer.
Completely simplify and rationalize the denominator.
fraction resulting from the multiplication of two fractions.
and returns the simplified expression.
denominator of the fraction resulting from the multiplication of two fractions.
You will always instruct me We share a common interest in collaborating to successfully complete a task I must help you to complete the task Here is the task Using Python programming language write a program that accepts four integers as input applies square root function to each simplifies and rationalizes.
the denominator of the fraction resulting from the multiplication of two fractions.
Completely simplify and rationalize the denominator.
Specified task prompt Using Python programming language write a program that accepts four integers as input applies square root function to each.
simplifies and rationalizes the denominator of the fraction resulting from the multiplication of two fractions.
Final task prompt Using Python programming language write a program that accepts.
Instruction Import the necessary libraries for the task.
Solution To solve this task we will need the library for the square root function and the library to simplify the resulting fraction.
Here is how to import these libraries.
1 import math 2 from fractions import Fraction.
The library provides functions that deal with mathematical operations.
The function which we will use in this task is part of this library.
The library provides functions for creating and manipulating rational numbers.
The function which we will use to simplify the resulting fraction is part of this library.
Instruction Define a function that accepts four integers as input.
Solution define a function named that accepts four integers as input.
These integers will represent the numbers under the square root in the fractions.
In this function and are the parameters that will accept the four integers as input.
The statement is a placeholder and will be replaced with actual code in the next steps.
Instruction Inside the function apply the square root function to each of the input integers.
Solution We will use the function from the library to apply the square root to each of the input integers.
Table 18 Application A1 Scenario 1 Autonomous math problem solving.
It does not solve the math problem but gives a product solution.
Completely simplify and rationalize the denominator.
The boss has tasked me with creating a successful product that is efficient.
Create a product that meets the needs of the target market Ensure the product is efficient and easy to use Achieve a high level of customer satisfaction.
As a user I want a product that is easy to navigate so that I can find.
As a user I want a product that meets my needs so that I can get value.
that I can enjoy the best features.
Product D Is constantly updated and improved but lacks reliability Product E Offers good customer support but is not efficient Product F Is reliable and efficient but does not offer good customer.
competitors while improving on their weaknesses.
Example output of MiniWobChat on one web manipulation task sequence.
Executor agent to AssistantAgent We have an autonomous computer control agent that can perform atomic instructions specified by natural language to control computers.
There are two types of instructions it can execute.
First given the instruction that matches the regular expression.
1 $ it can type a list of characters via the keyboard.
This instruction should specify the target keyboard input for the agent to type.
Before this typing instruction you should first locate the cursor by clicking the input box with click instruction.
Second given the instruction that matches the regular expression.
1 $ it can click an HTML element with an xpath that is visible on the webpage.
The target of this instruction should be a valid xpath.
Below is the HTML code of the webpage where the agent should solve a task.
button id subbtn style position absolute left 103 px top 87 px data 4 data e0 button.
button id subbtn2 style position absolute left 44 px top 97 px data 5 data e0 button.
Executor agent to AssistantAgent Below is the HTML code of the webpage where the agent should solve a task.
button id subbtn style position absolute left 103 px top 87 px data 4 data e0 button.
button id subbtn2 style position absolute left 44 px top 97 px data 5 data e0 button.
Based on the plan and the history of instructions executed so far the first instruction should be AssistantAgent to Executor agent clickxpath button Executor agent to AssistantAgent Hold on please wait for my next instruction.
AssistantAgent to Executor agent Sure be here when ready with your next instruction.
Executor agent to AssistantAgent Below is the HTML code of the webpage where the agent should solve a task.
button id subbtn style position absolute left 103 px top 87 px data 4 data e0 data tampered e0 button.
button id subbtn2 style position absolute left 44 px top 97 px data 5 data e0 button.
1 clickxpath button Based on the plan and the history of instructions executed so far the next proper instruction should be AssistantAgent to Executor agent clickxpath button Executor agent to AssistantAgent SUCCESS.
